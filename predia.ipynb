{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9922448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas removidas devido a mais de 20% de valores ausentes: ['OBES_IMC', 'PUERPERA', 'SIND_DOWN', 'HEPATICA', 'IMUNODEPRE', 'RENAL', 'PNEUMOPATI', 'OBESIDADE', 'FADIGA', 'DIABETES', 'CARDIOPATI', 'FATOR_RISC', 'CS_ESCOL_N', 'VOMITO', 'DIARREIA', 'GARGANTA', 'VACINA']\n",
      "Iniciando a seleção de features...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.90 MiB for an array with shape (498320,) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 254\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# O bloco de código abaixo só será executado se os arquivos forem encontrados\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m X_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# Seleção de Features\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m     selected_cols \u001b[38;5;241m=\u001b[39m features_selection(X_train, y_train)\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Validação Cruzada com o Modelo e Features Selecionadas ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    258\u001b[0m     final_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 189\u001b[0m, in \u001b[0;36mfeatures_selection\u001b[1;34m(X_train, y_train, num_features)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIniciando a seleção de features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    184\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\n\u001b[0;32m    185\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[0;32m    186\u001b[0m     subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m    187\u001b[0m )\n\u001b[1;32m--> 189\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    191\u001b[0m import_feature \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m    194\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features mais importantes são:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1664\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m   1661\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1662\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1665\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1666\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   1667\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   1668\u001b[0m     group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1669\u001b[0m     qid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1670\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1671\u001b[0m     base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m   1672\u001b[0m     feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[0;32m   1673\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39meval_set,\n\u001b[0;32m   1674\u001b[0m     sample_weight_eval_set\u001b[38;5;241m=\u001b[39msample_weight_eval_set,\n\u001b[0;32m   1675\u001b[0m     base_margin_eval_set\u001b[38;5;241m=\u001b[39mbase_margin_eval_set,\n\u001b[0;32m   1676\u001b[0m     eval_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1677\u001b[0m     eval_qid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1678\u001b[0m     create_dmatrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dmatrix,\n\u001b[0;32m   1679\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[0;32m   1680\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1681\u001b[0m )\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1684\u001b[0m     params,\n\u001b[0;32m   1685\u001b[0m     train_dmatrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1694\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1695\u001b[0m )\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:628\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    609\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    625\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m create_dmatrix(\n\u001b[0;32m    629\u001b[0m         data\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    630\u001b[0m         label\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    631\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[0;32m    632\u001b[0m         qid\u001b[38;5;241m=\u001b[39mqid,\n\u001b[0;32m    633\u001b[0m         weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    634\u001b[0m         base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m    635\u001b[0m         feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[0;32m    636\u001b[0m         missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    637\u001b[0m         enable_categorical\u001b[38;5;241m=\u001b[39menable_categorical,\n\u001b[0;32m    638\u001b[0m         feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m    639\u001b[0m         ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    640\u001b[0m     )\n\u001b[0;32m    642\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1137\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m QuantileDMatrix(\n\u001b[0;32m   1138\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, ref\u001b[38;5;241m=\u001b[39mref, nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1139\u001b[0m         )\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:1614\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1595\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1607\u001b[0m         )\n\u001b[0;32m   1608\u001b[0m     ):\n\u001b[0;32m   1609\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1612\u001b[0m         )\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(\n\u001b[0;32m   1615\u001b[0m     data,\n\u001b[0;32m   1616\u001b[0m     ref\u001b[38;5;241m=\u001b[39mref,\n\u001b[0;32m   1617\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[0;32m   1618\u001b[0m     weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[0;32m   1619\u001b[0m     base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m   1620\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[0;32m   1621\u001b[0m     qid\u001b[38;5;241m=\u001b[39mqid,\n\u001b[0;32m   1622\u001b[0m     label_lower_bound\u001b[38;5;241m=\u001b[39mlabel_lower_bound,\n\u001b[0;32m   1623\u001b[0m     label_upper_bound\u001b[38;5;241m=\u001b[39mlabel_upper_bound,\n\u001b[0;32m   1624\u001b[0m     feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[0;32m   1625\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m   1626\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m   1627\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39menable_categorical,\n\u001b[0;32m   1628\u001b[0m     max_quantile_blocks\u001b[38;5;241m=\u001b[39mmax_quantile_batches,\n\u001b[0;32m   1629\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:1678\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[0m\n\u001b[0;32m   1663\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1664\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread,\n\u001b[0;32m   1665\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1666\u001b[0m     max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin,\n\u001b[0;32m   1667\u001b[0m     max_quantile_blocks\u001b[38;5;241m=\u001b[39mmax_quantile_blocks,\n\u001b[0;32m   1668\u001b[0m )\n\u001b[0;32m   1669\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1670\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1671\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1677\u001b[0m )\n\u001b[1;32m-> 1678\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:572\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    570\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:553\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:640\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext(input_data)), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\data.py:1654\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1654\u001b[0m input_data(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m     new, cat_codes, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m     new, cat_codes, feature_names, feature_types \u001b[38;5;241m=\u001b[39m _proxy_transform(\n\u001b[0;32m    621\u001b[0m         data,\n\u001b[0;32m    622\u001b[0m         feature_names,\n\u001b[0;32m    623\u001b[0m         feature_types,\n\u001b[0;32m    624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_categorical,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\data.py:1707\u001b[0m, in \u001b[0;36m_proxy_transform\u001b[1;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_pa, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[0;32m   1706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[1;32m-> 1707\u001b[0m     df, feature_names, feature_types \u001b[38;5;241m=\u001b[39m _transform_pandas_df(\n\u001b[0;32m   1708\u001b[0m         data, enable_categorical, feature_names, feature_types\n\u001b[0;32m   1709\u001b[0m     )\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue type is not supported for data iterator:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\data.py:644\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[1;34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot have multiple columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    640\u001b[0m feature_names, feature_types \u001b[38;5;241m=\u001b[39m pandas_feature_info(\n\u001b[0;32m    641\u001b[0m     data, meta, feature_names, feature_types, enable_categorical\n\u001b[0;32m    642\u001b[0m )\n\u001b[1;32m--> 644\u001b[0m arrays \u001b[38;5;241m=\u001b[39m pandas_transform_data(data)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\data.py:604\u001b[0m, in \u001b[0;36mpandas_transform_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    602\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(arr)\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 604\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(oth_type(data[col]))\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# FIXME(jiamingy): Investigate the possibility of using dataframe protocol or arrow\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;66;03m# IPC format for pandas so that we can apply the data transformation inside XGBoost\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;66;03m# for better memory efficiency.\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\xgboost\\data.py:583\u001b[0m, in \u001b[0;36mpandas_transform_data.<locals>.oth_type\u001b[1;34m(ser)\u001b[0m\n\u001b[0;32m    579\u001b[0m     array \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;66;03m# Specifying the dtype can significantly slow down the conversion (about\u001b[39;00m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;66;03m# 15% slow down for dense inplace-predict)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m     array \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32, na_value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ensure_np_dtype(array, array\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\esthe\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:656\u001b[0m, in \u001b[0;36mIndexOpsMixin.to_numpy\u001b[1;34m(self, dtype, copy, na_value, **kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fillna:\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m can_hold_element(values, na_value):\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;66;03m# if we can't hold the na_value asarray either makes a copy or we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m         \u001b[38;5;66;03m# error before modifying values. The asarray later on thus won't make\u001b[39;00m\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;66;03m# another copy\u001b[39;00m\n\u001b[1;32m--> 656\u001b[0m         values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    658\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.90 MiB for an array with shape (498320,) and data type float32"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from xgboost import plot_tree\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ===================================================================\n",
    "# Funções de Auxílio\n",
    "# ===================================================================\n",
    "\n",
    "def is_number(s):\n",
    "    \"\"\"Verifica se um valor pode ser convertido para número.\"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "\n",
    "def col_miss(df):\n",
    "    \"\"\"Calcula a porcentagem de valores ausentes por coluna.\"\"\"\n",
    "    col_missing_df = df.isnull().sum(axis=0).reset_index()\n",
    "    col_missing_df.columns = ['col', 'missing_count']\n",
    "    col_missing_df['missing_part'] = col_missing_df['missing_count'] / len(df)\n",
    "    return col_missing_df.sort_values(by='missing_count', ascending=False)\n",
    "\n",
    "def plot_roc(labels, predict_prob, model_name, fig, labels_name, k):\n",
    "    \"\"\"Plota a curva ROC.\"\"\"\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(labels, predict_prob)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    line_list = ['--', '-']\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(false_positive_rate, true_positive_rate, line_list[k % 2], linewidth=1 + (1 - k / 5),\n",
    "            label=f'{model_name} AUC = {roc_auc:.4f}')\n",
    "    \n",
    "    plt.title('Curva ROC', fontsize=20)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.ylabel('Taxa de Verdadeiro Positivo (TPR)', fontsize=14)\n",
    "    plt.xlabel('Taxa de Falso Positivo (FPR)', fontsize=14)\n",
    "    labels_name.append(f'{model_name} AUC = {roc_auc:.4f}')\n",
    "    return labels_name\n",
    "\n",
    "def show_confusion_matrix(validations, predictions):\n",
    "    \"\"\"Exibe a matriz de confusão.\"\"\"\n",
    "    LABELS = ['Sobrevivência', 'Óbito']\n",
    "    matrix = confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.ylabel('Rótulo Verdadeiro')\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# Funções Principais de Pré-processamento e Modelagem\n",
    "# ===================================================================\n",
    "\n",
    "def data_preprocess(train_path, test_path, target_col='EVOLUCAO'):\n",
    "    \"\"\"\n",
    "    Carrega e pré-processa os dados de treino e teste.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        print(\"Erro: Arquivos 'train.csv' ou 'test.csv' não encontrados no caminho especificado.\")\n",
    "        print(f\"Por favor, verifique se a pasta 'data' está no mesmo diretório do seu script e contém os arquivos: {os.getcwd()}/data/\")\n",
    "        return None, None, None\n",
    "\n",
    "    # --- ALTERAÇÃO 1: low_memory=False para resolver problemas de tipo de dado\n",
    "    train_df = pd.read_csv(train_path, low_memory=False)\n",
    "    test_df = pd.read_csv(test_path, low_memory=False)\n",
    "    \n",
    "    # --- ALTERAÇÃO 2: Downcasting de tipos de dados para economizar memória ---\n",
    "    def downcast_df_types(df):\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'int64':\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            elif df[col].dtype == 'float64':\n",
    "                df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        return df\n",
    "\n",
    "    train_df = downcast_df_types(train_df)\n",
    "    test_df = downcast_df_types(test_df)\n",
    "    \n",
    "    # Identificar e Otimizar colunas de objeto\n",
    "    category_cols_to_convert = ['DT_NOTIFIC', 'CS_SEXO', 'ID_MN_RESI', 'SG_UF']\n",
    "    \n",
    "    for col in category_cols_to_convert:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df[col].astype('category')\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].astype('category')\n",
    "    \n",
    "    # Identificar e tratar colunas com muitos valores nulos\n",
    "    train_missing = col_miss(train_df)\n",
    "    cols_to_drop = train_missing[train_missing['missing_part'] > 0.20]['col'].tolist()\n",
    "    \n",
    "    if target_col in cols_to_drop:\n",
    "        cols_to_drop.remove(target_col)\n",
    "    \n",
    "    train_cols_to_drop = [col for col in cols_to_drop if col in train_df.columns]\n",
    "    test_cols_to_drop = [col for col in cols_to_drop if col in test_df.columns]\n",
    "\n",
    "    print(f\"Colunas removidas devido a mais de 20% de valores ausentes: {cols_to_drop}\")\n",
    "    train_df = train_df.drop(columns=train_cols_to_drop)\n",
    "    test_df = test_df.drop(columns=test_cols_to_drop)\n",
    "    \n",
    "    train_categorical_cols = train_df.select_dtypes(include='category').columns\n",
    "    test_categorical_cols = test_df.select_dtypes(include='category').columns\n",
    "\n",
    "    train_df[train_categorical_cols] = train_df[train_categorical_cols].astype('object')\n",
    "    test_df[test_categorical_cols] = test_df[test_categorical_cols].astype('object')\n",
    "    \n",
    "    train_df = train_df.fillna(-1)\n",
    "    test_df = test_df.fillna(-1)\n",
    "\n",
    "    categorical_cols_train = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_cols_test = test_df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    train_df = pd.get_dummies(train_df, columns=categorical_cols_train, drop_first=True)\n",
    "    test_df = pd.get_dummies(test_df, columns=categorical_cols_test, drop_first=True)\n",
    "    \n",
    "    train_cols = train_df.drop(columns=[target_col]).columns\n",
    "    test_cols = test_df.columns\n",
    "    \n",
    "    missing_in_test = list(set(train_cols) - set(test_cols))\n",
    "    for col in missing_in_test:\n",
    "        test_df[col] = 0\n",
    "    \n",
    "    test_df = test_df[train_cols]\n",
    "    \n",
    "    X_train = train_df.drop(columns=[target_col])\n",
    "    y_train = train_df[target_col].astype(int)\n",
    "    \n",
    "    X_test = test_df\n",
    "    y_test = None\n",
    "    \n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "def StratifiedKFold_func(x, y, model, num_iter=10, score_type='auc'):\n",
    "    \"\"\"Executa a validação cruzada K-Fold estratificada.\"\"\"\n",
    "    acc_val = []\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(x, y):\n",
    "        x_train, y_train = x.iloc[train_idx], y.iloc[train_idx]\n",
    "        x_val, y_val = x.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        if score_type == 'auc':\n",
    "            pred_proba = model.predict_proba(x_val)[:, 1]\n",
    "            acc_val.append(roc_auc_score(y_val, pred_proba))\n",
    "        else:\n",
    "            pred = model.predict(x_val)\n",
    "            acc_val.append(f1_score(y_val, pred))\n",
    "    \n",
    "    return np.mean(acc_val), np.std(acc_val)\n",
    "\n",
    "def features_selection(X_train, y_train, num_features=10):\n",
    "    \"\"\"Seleciona as 10 principais features usando a importância do XGBoost.\"\"\"\n",
    "    print(\"Iniciando a seleção de features...\")\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.2, reg_lambda=1, n_estimators=150,\n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    import_feature = pd.DataFrame({\n",
    "        'col': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    print(f\"As {num_features} features mais importantes são:\")\n",
    "    print(import_feature.head(num_features))\n",
    "    \n",
    "    top_cols = import_feature['col'].head(num_features).tolist()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='importance', y='col', data=import_feature.head(num_features))\n",
    "    plt.title('Importância das Features (XGBoost)', fontsize=16)\n",
    "    plt.xlabel('Importância Relativa', fontsize=12)\n",
    "    plt.ylabel('Features', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_cols\n",
    "\n",
    "def train_and_predict(X_train, y_train, X_test, selected_cols):\n",
    "    \"\"\"\n",
    "    Treina o modelo final com as features selecionadas e gera as previsões para o arquivo de teste.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando o treinamento do modelo final e a previsão...\")\n",
    "    \n",
    "    X_train_sel = X_train[selected_cols]\n",
    "    X_test_sel = X_test[selected_cols]\n",
    "\n",
    "    final_model = xgb.XGBClassifier(\n",
    "        max_depth=4, learning_rate=0.2, reg_lambda=1, n_estimators=150,\n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=42\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_sel, y_train)\n",
    "    \n",
    "    predictions = final_model.predict(X_test_sel)\n",
    "    \n",
    "    submission_df = pd.DataFrame({'EVOLUCAO': predictions})\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(\"Arquivo 'submission.csv' gerado com sucesso!\")\n",
    "    \n",
    "    return final_model, submission_df\n",
    "\n",
    "# ===================================================================\n",
    "# Execução Principal\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    TRAIN_PATH = 'data/train.csv'\n",
    "    TEST_PATH = 'data/test.csv'\n",
    "    \n",
    "    X_train, y_train, X_test = data_preprocess(TRAIN_PATH, TEST_PATH)\n",
    "\n",
    "    if X_train is not None and y_train is not None and X_test is not None:\n",
    "        selected_cols = features_selection(X_train, y_train)\n",
    "\n",
    "        print(\"\\n--- Validação Cruzada com o Modelo e Features Selecionadas ---\")\n",
    "        \n",
    "        final_model = xgb.XGBClassifier(random_state=42)\n",
    "        mean_f1, std_f1 = StratifiedKFold_func(X_train[selected_cols], y_train, final_model, score_type='f1')\n",
    "        \n",
    "        print(f\"F1-Score Médio (validação cruzada): {mean_f1:.4f} (+/- {std_f1:.4f})\")\n",
    "\n",
    "        model, submission = train_and_predict(X_train, y_train, X_test, selected_cols)\n",
    "        \n",
    "        print(\"\\n--- Previsões geradas com sucesso. Seu arquivo de submissão está pronto! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
